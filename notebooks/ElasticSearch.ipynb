{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Elasticsearch\n",
        "\n",
        "Elasticsearch is a search system based on tokens. Queries and documents are analyzed into tokens, and the most relevant query-document matches are calculated using a scoring algorithm. The default scoring algorithm is [BM25](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables). Powerful queries can be constructed using a [rich query syntax](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#query-string-syntax)."
      ],
      "metadata": {
        "id": "04sPbYG95gcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install elasticsearch==7.14.0\n",
        "!apt install default-jdk > /dev/null"
      ],
      "metadata": {
        "id": "I4DOFaOwHcvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import elasticsearch\n",
        "from elasticsearch import Elasticsearch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import json\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "from elasticsearch import helpers\n",
        "from pathlib import Path\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "vaIiKMw4MP-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download & extract Elasticsearch 7.0.0\n",
        "\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.0-linux-x86_64.tar.gz -q\n",
        "!tar -xzf elasticsearch-7.0.0-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0"
      ],
      "metadata": {
        "id": "yoWKYlpOMU9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A daemon instance of Elasticsearch refers to running the Elasticsearch server in the background as a daemon process. This allows Elasticsearch to continue running independently of any terminal or user session. Here's a detailed explanation:\n",
        "\n",
        "### What is a Daemon?\n",
        "\n",
        "A daemon is a background process that runs continuously and typically performs system-level tasks. Daemons do not require user interaction and are usually started at system boot and run until the system is shut down. They operate silently in the background, handling tasks like logging, scheduling, or, in this case, search indexing and querying.\n",
        "\n",
        "### Why Run Elasticsearch as a Daemon?\n",
        "\n",
        "Running Elasticsearch as a daemon has several advantages:\n",
        "\n",
        "1. **Continuous Operation**: Elasticsearch can run continuously, handling requests and managing indexes without interruption, even if users log out or the terminal session ends.\n",
        "2. **Resource Management**: Running in the background helps in better resource management and allows the system to allocate resources efficiently.\n"
      ],
      "metadata": {
        "id": "3hPFQs_magr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating daemon instance of elasticsearch\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.0.0/bin/elasticsearch'],\n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                 )"
      ],
      "metadata": {
        "id": "Rr1GJHOUMVIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This part is important, since it takes a little amount of time for instance to load\n",
        "import time\n",
        "time.sleep(20)"
      ],
      "metadata": {
        "id": "KmCCXBaFMfdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# If you get 1 root & 2 daemon process then Elasticsearch instance has started successfully\n",
        "ps -ef | grep elasticsearch"
      ],
      "metadata": {
        "id": "Ik5A4z9PMgvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if elasticsearch is running\n",
        "!curl -sX GET \"localhost:9200/\""
      ],
      "metadata": {
        "id": "OkI7EjdLMmcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = Elasticsearch(hosts = [{\"host\":\"localhost\", \"port\":9200}])\n",
        "# Check if python is connected to elasticsearch\n",
        "es.ping()"
      ],
      "metadata": {
        "id": "YDmlGiaTMrsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    tarball_path = Path(\"/content/datasets/articles_es.tgz\")\n",
        "    if not tarball_path.is_file():\n",
        "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
        "        url = \"https://github.com/tarekhaledai/elasticsearch/raw/main/data/articles_es.tgz\"\n",
        "        urllib.request.urlretrieve(url, tarball_path)\n",
        "        with tarfile.open(tarball_path) as dataset_tarball:\n",
        "            dataset_tarball.extractall(path=\"datasets\")\n",
        "    return pd.read_csv(Path(\"/content/datasets/articles_es.csv\"))\n",
        "\n",
        "dataset = load_data()\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "fbtkbeOPMunT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define settings & mappings of Elasticsearch index\n",
        "Settings = {\n",
        "    \"settings\":{\n",
        "        \"number_of_shards\":1,\n",
        "        \"number_of_replicas\":0\n",
        "    },\n",
        "    \"mappings\":{\n",
        "        \"properties\":{\n",
        "            \"article\":{\n",
        "                \"type\":\"text\"\n",
        "            },\n",
        "            \"highlights\":{\n",
        "                \"type\":\"text\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "7Pafya86M1Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Settings` dictionary provided defines the configuration for an Elasticsearch index. This configuration includes settings for the number of shards and replicas, as well as mappings that specify the structure of the documents within the index. Here's a detailed description of each component:\n",
        "\n",
        "### Settings\n",
        "\n",
        "The `settings` key in the dictionary configures how Elasticsearch will handle the index in terms of distribution and redundancy.\n",
        "\n",
        "- `\"number_of_shards\": 1`\n",
        "  - **Number of Shards**: Shards are subdivisions of an index that allow for distributed storage and parallel processing of data. Setting this to 1 means the index will have a single shard. This is suitable for small datasets or development environments where high scalability is not required.\n",
        "\n",
        "- `\"number_of_replicas\": 0`\n",
        "  - **Number of Replicas**: Replicas are copies of the primary shards and provide redundancy and fault tolerance. Setting this to 0 means there will be no replicas, which implies no redundancy. This setting might be used in a development environment where data loss is acceptable, or to save resources.\n",
        "\n",
        "### Mappings\n",
        "\n",
        "The `mappings` key defines the structure and data types of the documents stored in the index. This is crucial for Elasticsearch to understand how to index and search the data.\n",
        "\n",
        "- `\"properties\"`: This defines the fields within the documents and their respective data types.\n",
        "\n",
        "  - `\"article\": {\"type\": \"text\"}`\n",
        "    - **Article Field**: This field will store the main content of the document. The type is set to `text`, which means Elasticsearch will analyze the field and create an inverted index to support full-text search capabilities. This is suitable for fields where you want to perform search queries on the content.\n",
        "\n",
        "  - `\"highlights\": {\"type\": \"text\"}`\n",
        "    - **Highlights Field**: Similar to the `article` field, this field is also of type `text`. This indicates it will store textual data that is likely intended to be searchable and analyzed in the same way as the `article` field.\n",
        "\n"
      ],
      "metadata": {
        "id": "LXQ9-4DQunuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def json_formatter(dataset, index_name, index_type='_doc'):\n",
        "    \"\"\"\n",
        "    This function is used to create JSON formatted dictionaries for Elasticsearch.\n",
        "\n",
        "    Args:\n",
        "      dataset: The dataset you want to apply this function.\n",
        "      index_name: Name of the index in Elasticsearch\n",
        "      index_type: Type of the index in Elasticsearch.\n",
        "      Note: It is suggested to keep index_type as '_doc' since it is deprecated from version 6.\n",
        "      Note: This function formats all columns of your dataset, if you want to apply this to special columns only,\n",
        "      you can delete the second for loop and add your custom fields.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        List = []\n",
        "        columns = dataset.columns\n",
        "        for idx, row in dataset.iterrows():\n",
        "            dic = {}\n",
        "            dic['_index'] = index_name\n",
        "            dic['_type'] = index_type\n",
        "            source = {}\n",
        "            for i in dataset.columns:\n",
        "                source[i] = row[i]\n",
        "            dic['_source'] = source\n",
        "            List.append(dic)\n",
        "        return List\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"There is a problem: {}\".format(e))"
      ],
      "metadata": {
        "id": "SYjWv8w2NZ3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MY_INDEX = es.indices.create(index=\"news_index\", ignore=[400,404], body=Settings)\n",
        "MY_INDEX"
      ],
      "metadata": {
        "id": "q0Q9DFl7NgKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_Formatted_dataset = json_formatter(dataset=dataset, index_name='news_index', index_type='_doc')\n",
        "json_Formatted_dataset[0]"
      ],
      "metadata": {
        "id": "BguylLPqNlAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For importing Data to elasticsearch we use elasticsearch's bulk API from elasticsearch.helpers\n",
        "try:\n",
        "    res = helpers.bulk(es, json_Formatted_dataset)\n",
        "    print(\"successfully imported to elasticsearch.\")\n",
        "except Exception as e:\n",
        "    print(f\"error: {e}\")"
      ],
      "metadata": {
        "id": "H9YxpizrNr13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10 sample of data\n",
        "query = es.search(\n",
        "    index=\"news_index\",\n",
        "    body={\n",
        "      \"size\":10,\n",
        "      \"query\": {\n",
        "        \"match_all\":{}\n",
        "      }\n",
        "    }\n",
        ")\n",
        "\n",
        "output = pd.json_normalize((query['hits']['hits']))\n",
        "output"
      ],
      "metadata": {
        "id": "qUuBF-rpN6IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complicated query\n",
        "query = es.search(\n",
        "    index=\"news_index\",\n",
        "    body={\n",
        "        \"size\":20,\n",
        "        \"query\":{\n",
        "            \"bool\":{\n",
        "                \"must\":[\n",
        "                        {\"match\":{\"article\":\"dogs fight\"}}\n",
        "                ],\n",
        "                \"should\":[\n",
        "                        {\"match\":{\"highlights\":\"cat\"}}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "output = pd.json_normalize((query['hits']['hits']))\n",
        "output"
      ],
      "metadata": {
        "id": "Mv4HPowmOEgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More complicated query\n",
        "query = es.search(\n",
        "    index=\"news_index\",\n",
        "    body={\n",
        "        \"size\":20,\n",
        "        \"query\":{\n",
        "            \"bool\":{\n",
        "                \"must\":[\n",
        "                        {\"multi_match\":{\n",
        "                            \"query\":\"Football manchester united\",\n",
        "                            \"fields\":[\"article\",\"highlights\"]\n",
        "                        }}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "output = pd.json_normalize((query['hits']['hits']))\n",
        "output"
      ],
      "metadata": {
        "id": "OFshV3IeOS6A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}